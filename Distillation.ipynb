{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lenovo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\datapoints\\__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "c:\\Users\\lenovo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\transforms\\v2\\__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision.transforms import v2\n",
    "from torchvision.models import efficientnet_b0,EfficientNet_B0_Weights,densenet121,DenseNet121_Weights\n",
    "from torch.utils.data import DataLoader\n",
    "import skorch\n",
    "from skorch.helper import predefined_split\n",
    "from skorch.callbacks import Checkpoint,Freezer\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score,f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KONet(torch.nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            m1_ratio=0.6,\n",
    "            m2_ratio=0.4,\n",
    "            m1_dropout=0.1,\n",
    "            m2_dropout=0.3,\n",
    "            n_classes=2\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert m1_ratio+m2_ratio==1\n",
    "        self.n_classes=n_classes\n",
    "        self.m1_ratio=m1_ratio\n",
    "        self.m2_ratio=m2_ratio\n",
    "        self.m1_dropout=m1_dropout\n",
    "        self.m2_dropout=m2_dropout\n",
    "\n",
    "        self.efficient=efficientnet_b0(weights=EfficientNet_B0_Weights.DEFAULT)\n",
    "        self.efficient.classifier[0]=torch.nn.Dropout(p=self.m1_dropout,inplace=True)\n",
    "        self.efficient.classifier[-1]=torch.nn.Linear(in_features=1280,out_features=self.n_classes)\n",
    "\n",
    "        self.dense=densenet121(weights=DenseNet121_Weights.DEFAULT)\n",
    "        self.dense.classifier=torch.nn.Sequential(torch.nn.Dropout(p=self.m2_dropout,inplace=True),\n",
    "                                            torch.nn.Linear(in_features=1024,out_features=n_classes),\n",
    "                                            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        m1=self.efficient(x)\n",
    "        m2=self.dense(x)\n",
    "        out=self.m1_ratio*m1+self.m2_ratio*m2\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes=2\n",
    "image_shape=224\n",
    "augmented_dataset_size=4000\n",
    "path=\"D:\\Osteoporosis detection\\datasets\\Osteoporosis Knee X-ray modified\\Osteoporosis Knee X-ray\"\n",
    "non_augment_transform=v2.Compose([v2.ToImageTensor(),\n",
    "                       v2.ToDtype(torch.float32),\n",
    "                       v2.Resize((image_shape,image_shape),antialias=True),\n",
    "                       v2.Normalize(mean=[0],std=[1]),\n",
    "                       ])\n",
    "transforms=v2.Compose([v2.ToImageTensor(),\n",
    "                       v2.ToDtype(torch.float32),\n",
    "                       v2.RandomAffine(degrees=30,shear=30),\n",
    "                       v2.RandomZoomOut(side_range=(1,1.5)),\n",
    "                       v2.Resize((image_shape,image_shape),antialias=True),\n",
    "                       v2.Normalize(mean=[0],std=[1]),\n",
    "                       ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_augmented_dataset=torchvision.datasets.ImageFolder(path,transform=non_augment_transform)\n",
    "dataset=torchvision.datasets.ImageFolder(path,transform=transforms)\n",
    "factor=augmented_dataset_size//len(dataset)-1\n",
    "\n",
    "new_dataset=torch.utils.data.ConcatDataset([non_augmented_dataset]+[dataset for _ in range(factor)])\n",
    "del non_augmented_dataset,dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset=torchvision.datasets.ImageFolder(path,transform=transforms)\n",
    "generator1 = torch.Generator().manual_seed(42)\n",
    "train_split=0.2\n",
    "valid_split=0.1\n",
    "test_split=0.7\n",
    "train_set,valid_set,test_set=torch.utils.data.random_split(new_dataset, [train_split,valid_split,test_split],\n",
    "                                                            generator=generator1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class distiller(torch.nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            large_model,\n",
    "            small_model\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.large_model=large_model\n",
    "        self.small_model=small_model\n",
    "\n",
    "    def forward(self, x):\n",
    "        large_output=self.large_model(x)\n",
    "        small_output=self.small_model(x)\n",
    "        print(large_output)\n",
    "        print(small_output)\n",
    "        return (small_output,large_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name='distiller'\n",
    "large_model_name='dense'\n",
    "small_model_name='conv_next'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Large model initiallization\n",
    "if large_model_name=='dense':\n",
    "    large_model=densenet121(weights=DenseNet121_Weights.DEFAULT)\n",
    "    p=0.3\n",
    "    large_model.classifier=torch.nn.Sequential(torch.nn.Dropout(p=p,inplace=True),\n",
    "                                        torch.nn.Linear(in_features=1024,out_features=n_classes),\n",
    "                                        )\n",
    "elif large_model_name=='KONetOtherFinetuned':\n",
    "    m1_ratio=0.6\n",
    "    m2_ratio=0.4\n",
    "    m1_dropout=0.1\n",
    "    m2_dropout=0.3\n",
    "    large_model=KONet(m1_ratio=m1_ratio,m2_ratio=m2_ratio,m1_dropout=m1_dropout,m2_dropout=m2_dropout,n_classes=n_classes)\n",
    "#model.load_state_dict(torch.load(f'model/{model_name}best_param.pkl'))\n",
    "if small_model_name=='conv_next':\n",
    "    p=0.3\n",
    "    small_model=torchvision.models.convnext_tiny(weights='DEFAULT')\n",
    "    small_model.classifier[2]=torch.nn.Sequential(torch.nn.Dropout(p=p,inplace=True),\n",
    "                                        torch.nn.Linear(in_features=768,out_features=n_classes),\n",
    "                                        )\n",
    "model=distiller(large_model=large_model,small_model=small_model)\n",
    "#Freeze entirety of large model so only small model changes\n",
    "freeze=['large_model.*.weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "from torch import Tensor\n",
    "from torch.nn.modules.loss import _WeightedLoss\n",
    "from torch.nn import functional as F\n",
    "#Now we need to create our own loss function which will perform cross entropy loss\n",
    "class distill_loss(_WeightedLoss):\n",
    "    __constants__ = ['ignore_index', 'reduction', 'label_smoothing']\n",
    "    ignore_index: int\n",
    "    label_smoothing: float\n",
    "\n",
    "    def __init__(self, weight: Optional[Tensor] = None, size_average=None, ignore_index: int = -100,\n",
    "                 reduce=None, reduction: str = 'mean', label_smoothing: float = 0.0, T:int = 2,\n",
    "                 soft_target_loss_weight: float = 0.25, ce_loss_weight: float = 0.75,) -> None:\n",
    "        super().__init__(weight, size_average, reduce, reduction)\n",
    "        self.ignore_index = ignore_index\n",
    "        self.label_smoothing = label_smoothing\n",
    "        self.T=T\n",
    "        self.soft_target_loss_weight=soft_target_loss_weight\n",
    "        self.ce_loss_weight=ce_loss_weight\n",
    "\n",
    "    def forward(self, input: Tensor, target: Tensor) -> Tensor:\n",
    "        soft_targets = F.softmax(input[1] / self.T, dim=-1)\n",
    "        soft_prob = F.softmax(input[0] / self.T, dim=-1)\n",
    "        #print(soft_targets)\n",
    "        #print(soft_prob)\n",
    "        soft_targets_loss = -torch.sum(soft_targets * (soft_prob.log())) / soft_prob.size()[0] * (self.T**2)\n",
    "        #print(soft_targets_loss)\n",
    "        label_loss = F.cross_entropy(input[0], target, weight=self.weight,\n",
    "                               ignore_index=self.ignore_index, reduction=self.reduction,\n",
    "                               label_smoothing=self.label_smoothing)  \n",
    "        #print(label_loss) \n",
    "        loss = self.soft_target_loss_weight * soft_targets_loss + self.ce_loss_weight * label_loss\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "monitor = lambda net: any(net.history[-1, ('valid_accuracy_best','valid_loss_best')])\n",
    "cp=Checkpoint(monitor='valid_loss_best',dirname='model',f_params=f'{model_name}best_param.pkl',\n",
    "               f_optimizer=f'{model_name}best_opt.pkl', f_history=f'{model_name}best_history.json')\n",
    "cb = skorch.callbacks.Freezer(freeze)\n",
    "classifier = skorch.NeuralNetClassifier(\n",
    "        model,\n",
    "        criterion=distill_loss(),\n",
    "        optimizer=torch.optim.AdamW,\n",
    "        train_split=predefined_split(valid_set),\n",
    "        iterator_train=DataLoader,\n",
    "        iterator_valid=DataLoader,\n",
    "        iterator_train__shuffle=True,\n",
    "        iterator_train__pin_memory=True,\n",
    "        iterator_valid__pin_memory=True,\n",
    "        #iterator_train__num_workers=1,\n",
    "        #iterator_valid__num_workers=1,\n",
    "        #iterator_train__persistent_workers=True,\n",
    "        #iterator_valid__persistent_workers=True,\n",
    "        batch_size=32,\n",
    "        device='cuda',\n",
    "        callbacks=[cp,cb],#Try to implement accuracy and f1 score callables here\n",
    "        warm_start=True,\n",
    "        )\n",
    "classifier.initialize()\n",
    "classifier.module_.large_model.load_state_dict(torch.load(f'model/{large_model_name}best_param.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2.1463, -2.1091]], device='cuda:0')\n",
      "tensor([[0.0024, 0.2747]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "test=np.ones((1,3,image_shape,image_shape),dtype=np.float32)\n",
    "out=classifier.predict_proba(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.fit(train_set,y=None,epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.load_params(f_params='model/distiller_otherbest_param.pkl')\n",
    "distilled_model=classifier.module_.small_model\n",
    "classifier.module_=distilled_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'conv_next'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.save_params(f_params=f'model/{small_model_name}_distilled_otherbest_param.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "conv_next\n",
      "Accuracy mean: 0.9987734487734488 standard deviation: 0.0004893455976280795\n",
      "F1-Score mean: 0.998925789594517 standard deviation: 0.0004294201753277975\n",
      "ROC_AUC  mean: 0.9999944733530592 standard deviation: 8.16780007857457e-06\n"
     ]
    }
   ],
   "source": [
    "iterations=5\n",
    "accuracy=[]\n",
    "f1=[]\n",
    "auc=[]\n",
    "test_loader=DataLoader(test_set,batch_size=8,shuffle=False,num_workers=4,pin_memory=True,persistent_workers=True)\n",
    "for i in range(iterations):\n",
    "    print(i)\n",
    "    probs=[]\n",
    "    actual_labels=[]\n",
    "    for test_features, actual_lb in iter(test_loader):\n",
    "        prob=classifier.predict_proba(test_features)\n",
    "        actual_lb=np.array(actual_lb)\n",
    "        probs.append(prob)\n",
    "        actual_labels.append(actual_lb)\n",
    "\n",
    "    probs=np.concatenate(probs)\n",
    "    pred_labels=np.argmax(probs,axis=1)\n",
    "    actual_labels=np.concatenate(actual_labels)\n",
    "\n",
    "    iteration_auc=roc_auc_score(actual_labels,probs[:,1])\n",
    "    iteration_accuracy=np.mean(pred_labels==actual_labels)\n",
    "    iteration_f1=f1_score(actual_labels,pred_labels)\n",
    "\n",
    "    accuracy.append(iteration_accuracy)\n",
    "    f1.append(iteration_f1)\n",
    "    auc.append(iteration_auc)\n",
    "\n",
    "print(small_model_name)\n",
    "\n",
    "print(f\"Accuracy mean: {np.mean(accuracy)} standard deviation: {np.std(accuracy)}\")\n",
    "print(f\"F1-Score mean: {np.mean(f1)} standard deviation: {np.std(f1)}\")\n",
    "print(f\"ROC_AUC  mean: {np.mean(auc)} standard deviation: {np.std(auc)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
