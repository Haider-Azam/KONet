{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision.transforms import v2\n",
    "from torchvision.models import efficientnet_b0,EfficientNet_B0_Weights,densenet121,DenseNet121_Weights\n",
    "from torch.utils.data import DataLoader\n",
    "import skorch\n",
    "from skorch.helper import predefined_split\n",
    "from skorch.callbacks import Checkpoint,Freezer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data first preprocessed and then split into train/val/test with ratios 0.8/0.1/0.1\n",
    "#Data preprocessing is augmentation and normalization. The data will be resized according to model input size then\n",
    "#Rotation, shearing and zooming were used to increase dataset size from 372 to 4000 images. Then normalization was applied and\n",
    "#Dataset was split using ratio 0.8/0.1/0.1 for train/valid/test set. Split-folder python library was used.\n",
    "#First 10 layers of models were frozen, last 10 layers will be fine-tuned and final classifier layer changed to linear layer\n",
    "\n",
    "#Models trained individually and then added to KONet, efficientnetbo dropout ratio of 0.1 and densenet121 dropout ratio of 0.3\n",
    "\n",
    "#First 10 layers will be frozen and the rest will be fine tuned but paper says only last 10 layers will be fine tuned.\n",
    "#Unknown what is considered as a layer and what isn't.\n",
    "#We will instead freeze roughly the first half of the layers in the model.\n",
    "#We will freeze first Conv2dNormActivation and first nine MBConv blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes=2\n",
    "image_shape=224\n",
    "augmented_dataset_size=4000\n",
    "path=\"D:\\Osteoporosis detection\\datasets\\Osteoporosis Knee X-ray modified\\Osteoporosis Knee X-ray\"\n",
    "non_augment_transform=v2.Compose([v2.ToImageTensor(),\n",
    "                       v2.ToDtype(torch.float32),\n",
    "                       v2.Resize((image_shape,image_shape),antialias=True),\n",
    "                       v2.Normalize(mean=[0],std=[1]),\n",
    "                       ])\n",
    "transforms=v2.Compose([v2.ToImageTensor(),\n",
    "                       v2.ToDtype(torch.float32),\n",
    "                       v2.RandomAffine(degrees=30,shear=30),\n",
    "                       v2.RandomZoomOut(side_range=(1,1.5)),\n",
    "                       v2.Resize((image_shape,image_shape),antialias=True),\n",
    "                       v2.Normalize(mean=[0],std=[1]),\n",
    "                       ])\n",
    "\n",
    "non_augmented_dataset=torchvision.datasets.ImageFolder(path,transform=non_augment_transform)\n",
    "dataset=torchvision.datasets.ImageFolder(path,transform=non_augment_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factor=augmented_dataset_size//len(dataset)-1\n",
    "new_dataset=torch.utils.data.ConcatDataset([non_augmented_dataset]+[dataset for _ in range(factor)])\n",
    "del non_augmented_dataset,dataset\n",
    "\n",
    "#dataset=torchvision.datasets.ImageFolder(path,transform=transforms)\n",
    "generator1 = torch.Generator().manual_seed(42)\n",
    "train_split=0.2\n",
    "valid_split=0.1\n",
    "test_split=0.7\n",
    "train_set,valid_set,test_set=torch.utils.data.random_split(new_dataset, [train_split,valid_split,test_split],\n",
    "                                                            generator=generator1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KONet(torch.nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            m1_ratio=0.6,\n",
    "            m2_ratio=0.4,\n",
    "            m1_dropout=0.1,\n",
    "            m2_dropout=0.3,\n",
    "            n_classes=2\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert m1_ratio+m2_ratio==1\n",
    "        self.n_classes=n_classes\n",
    "        self.m1_ratio=m1_ratio\n",
    "        self.m2_ratio=m2_ratio\n",
    "        self.m1_dropout=m1_dropout\n",
    "        self.m2_dropout=m2_dropout\n",
    "\n",
    "        self.efficient=efficientnet_b0(weights=EfficientNet_B0_Weights.DEFAULT)\n",
    "        self.efficient.classifier[0]=torch.nn.Dropout(p=self.m1_dropout,inplace=True)\n",
    "        self.efficient.classifier[-1]=torch.nn.Linear(in_features=1280,out_features=self.n_classes)\n",
    "\n",
    "        self.dense=densenet121(weights=DenseNet121_Weights.DEFAULT)\n",
    "        self.dense.classifier=torch.nn.Sequential(torch.nn.Dropout(p=self.m2_dropout,inplace=True),\n",
    "                                            torch.nn.Linear(in_features=1024,out_features=n_classes),\n",
    "                                            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        m1=self.efficient(x)\n",
    "        m2=self.dense(x)\n",
    "        out=self.m1_ratio*m1+self.m2_ratio*m2\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name='KONet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EfficientNetB0 has 16 MBConv layers, freeze till 8th MBConv layer then. Freeze all till before 5th sequential\n",
    "#DenseNet121 has 58 dense layers, freeze till 29th dense layer then. #Till before dense block 3\n",
    "#Conv next tiny has 18 CNBlock\n",
    "if model_name=='efficient':\n",
    "    model=efficientnet_b0(weights=EfficientNet_B0_Weights.DEFAULT)\n",
    "    p=0.1\n",
    "    model.classifier[0]=torch.nn.Dropout(p=p,inplace=True)\n",
    "    model.classifier[-1]=torch.nn.Linear(in_features=1280,out_features=n_classes)\n",
    "    frozen_layers=4\n",
    "    freeze=['features.{}*.weight'.format(i) for i in range(frozen_layers)] + ['features.{}*.bias'.format(i) for i in range(frozen_layers)]\n",
    "    \n",
    "elif model_name=='dense':\n",
    "    model=densenet121(weights=DenseNet121_Weights.DEFAULT)\n",
    "    p=0.3\n",
    "    model.classifier=torch.nn.Sequential(torch.nn.Dropout(p=p,inplace=True),\n",
    "                                         torch.nn.Linear(in_features=1024,out_features=n_classes),\n",
    "                                        )\n",
    "    freeze=['features.conv0.weight','features.conv0.bias','features.norm0.weight','features.norm0.bias',\n",
    "            'features.denseblock1.*.weight','features.denseblock1.*.bias','features.denseblock2.*.weight','features.denseblock2.*.bias',\n",
    "            ]\n",
    "    freeze+=['features.denseblock3.denselayer{}.*.weight'.format(i) for i in range(1,12)]\n",
    "    freeze+=['features.denseblock3.denselayer{}.*.bias'.format(i) for i in range(1,12)]\n",
    "\n",
    "elif model_name=='conv_next':\n",
    "    p=0.3\n",
    "    model=torchvision.models.convnext_tiny(weights='DEFAULT')\n",
    "    model.classifier[2]=torch.nn.Sequential(torch.nn.Dropout(p=p,inplace=True),\n",
    "                                        torch.nn.Linear(in_features=768,out_features=n_classes),\n",
    "                                        )\n",
    "    frozen_layers=5\n",
    "    freeze=['features.{}*.weight'.format(i) for i in range(frozen_layers)]\n",
    "    freeze+=['features.{}*.bias'.format(i) for i in range(frozen_layers)]\n",
    "\n",
    "    freeze=['features.5.{}*.weight'.format(i) for i in range(2)]\n",
    "    freeze+=['features.5.{}*.bias'.format(i) for i in range(2)]\n",
    "elif model_name=='KONet':\n",
    "    m1_ratio=0.6\n",
    "    m2_ratio=0.4\n",
    "    m1_dropout=0.1\n",
    "    m2_dropout=0.3\n",
    "    model=KONet(m1_ratio=m1_ratio,m2_ratio=m2_ratio,m1_dropout=m1_dropout,m2_dropout=m2_dropout,n_classes=n_classes)\n",
    "    #Defines the blocks to be frozen\n",
    "    m1_frozen_layers=4\n",
    "    freeze=['efficient.features.{}*.weight'.format(i) for i in range(m1_frozen_layers)]\n",
    "    freeze+=['efficient.features.{}*.bias'.format(i) for i in range(m1_frozen_layers)]\n",
    "\n",
    "    freeze+=['dense.features.conv0.weight','dense.features.conv0.bias','dense.features.norm0.weight','dense.features.norm0.bias',\n",
    "            'dense.features.denseblock1.*.weight','dense.features.denseblock1.*.bias','dense.features.denseblock2.*.weight',\n",
    "            'dense.features.denseblock2.*.bias',\n",
    "            ]\n",
    "    freeze+=['dense.features.denseblock3.denselayer{}.*.weight'.format(i) for i in range(1,12)]\n",
    "    freeze+=['dense.features.denseblock3.denselayer{}.*.bias'.format(i) for i in range(1,12)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monitor = lambda net: any(net.history[-1, ('valid_accuracy_best','valid_loss_best')])\n",
    "cp=Checkpoint(monitor='valid_loss_best',dirname='model',f_params=f'{model_name}best_param.pkl',\n",
    "               f_optimizer=f'{model_name}best_opt.pkl', f_history=f'{model_name}best_history.json')\n",
    "cb = skorch.callbacks.Freezer(freeze)\n",
    "classifier = skorch.NeuralNetClassifier(\n",
    "        model,\n",
    "        criterion=torch.nn.CrossEntropyLoss(),\n",
    "        optimizer=torch.optim.AdamW,\n",
    "        train_split=predefined_split(valid_set),\n",
    "        iterator_train=DataLoader,\n",
    "        iterator_valid=DataLoader,\n",
    "        iterator_train__shuffle=True,\n",
    "        iterator_train__pin_memory=True,\n",
    "        iterator_valid__pin_memory=True,\n",
    "        #iterator_train__num_workers=1,\n",
    "        #iterator_valid__num_workers=1,\n",
    "        #iterator_train__persistent_workers=True,\n",
    "        #iterator_valid__persistent_workers=True,\n",
    "        batch_size=32,\n",
    "        device='cuda',\n",
    "        callbacks=[cp,cb],#Try to implement accuracy and f1 score callables here\n",
    "        warm_start=True,\n",
    "        )\n",
    "classifier.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=torch.ones((3,3,image_shape,image_shape))\n",
    "out=classifier.predict(test)\n",
    "print(\"Output shape: \",out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.fit(train_set,y=None,epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score,f1_score\n",
    "import numpy as np\n",
    "classifier.load_params(f_params=f'model/{model_name}OtherFinetunedbest_param.pkl')\n",
    "print(\"Paramters Loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_model_name='efficient'\n",
    "if ensemble_model_name=='efficient':\n",
    "    ensemble_model=classifier.module_.efficient\n",
    "    classifier.module_=ensemble_model\n",
    "elif ensemble_model_name=='dense':\n",
    "    ensemble_model=classifier.module_.dense\n",
    "    classifier.module_=ensemble_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.save_params(f_params=f'model/{ensemble_model_name}OtherFinetunedbest_param.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KONet\n",
      "Accuracy mean: 0.5757575757575758 standard deviation: 0.0\n",
      "F1-Score mean: 0.7307692307692308 standard deviation: 0.0\n",
      "ROC_AUC  mean: 0.5071924603174603 standard deviation: 0.031894353083321086\n"
     ]
    }
   ],
   "source": [
    "iterations=5\n",
    "accuracy=[]\n",
    "f1=[]\n",
    "auc=[]\n",
    "test_loader=DataLoader(valid_set,batch_size=8,shuffle=False)\n",
    "for _ in range(iterations):\n",
    "    probs=[]\n",
    "    actual_labels=[]\n",
    "    for test_features, actual_lb in iter(test_loader):\n",
    "        prob=classifier.predict_proba(test_features)\n",
    "        actual_lb=np.array(actual_lb)\n",
    "        probs.append(prob)\n",
    "        actual_labels.append(actual_lb)\n",
    "\n",
    "    probs=np.concatenate(probs)\n",
    "    pred_labels=np.argmax(probs,axis=1)\n",
    "    actual_labels=np.concatenate(actual_labels)\n",
    "\n",
    "    iteration_auc=roc_auc_score(actual_labels,probs[:,1])\n",
    "    iteration_accuracy=np.mean(pred_labels==actual_labels)\n",
    "    iteration_f1=f1_score(actual_labels,pred_labels)\n",
    "\n",
    "    accuracy.append(iteration_accuracy)\n",
    "    f1.append(iteration_f1)\n",
    "    auc.append(iteration_auc)\n",
    "\n",
    "print(model_name)\n",
    "\n",
    "print(f\"Accuracy mean: {np.mean(accuracy)} standard deviation: {np.std(accuracy)}\")\n",
    "print(f\"F1-Score mean: {np.mean(f1)} standard deviation: {np.std(f1)}\")\n",
    "print(f\"ROC_AUC  mean: {np.mean(auc)} standard deviation: {np.std(auc)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
